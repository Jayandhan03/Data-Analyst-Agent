{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "991afff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def eda_fact_sheet_bakery(path: str, sample_size: int = 3, freq_top: int = 3):\n",
    "    \"\"\"\n",
    "    Generates a batchwise fact sheet for a dataset.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to CSV or Excel dataset.\n",
    "        sample_size (int): Number of example values per column.\n",
    "        freq_top (int): Number of top frequent values per column.\n",
    "\n",
    "    Returns:\n",
    "        dict: Fact sheet containing dataset-level info and column-level stats in batches.\n",
    "    \"\"\"\n",
    "    # --- Load dataset ---\n",
    "    if path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "    elif path.endswith(\".xlsx\") or path.endswith(\".xls\"):\n",
    "        df = pd.read_excel(path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use CSV or Excel.\")\n",
    "\n",
    "    n_rows, n_columns = df.shape\n",
    "\n",
    "    fact_sheet = {\n",
    "        \"n_rows\": n_rows,\n",
    "        \"n_columns\": n_columns,\n",
    "        \"batches\": []\n",
    "    }\n",
    "\n",
    "    # --- Determine batch splits ---\n",
    "    batch_size = 15\n",
    "    batch_splits = [df.columns[i:i+batch_size].tolist() for i in range(0, n_columns, batch_size)]\n",
    "\n",
    "    # --- Process each batch ---\n",
    "    for batch_cols in batch_splits:\n",
    "        batch_profile = {\"columns\": {}}\n",
    "        for col in batch_cols:\n",
    "            series = df[col]\n",
    "            col_profile = {}\n",
    "\n",
    "            total = len(series)\n",
    "\n",
    "            # Core stats\n",
    "            col_profile[\"dtype\"] = str(series.dtype)\n",
    "            col_profile[\"null_percent\"] = round(float(series.isna().sum() / total * 100), 2)\n",
    "            col_profile[\"unique_percent\"] = round(float(series.nunique(dropna=True) / total * 100), 2)\n",
    "\n",
    "            # Example values\n",
    "            try:\n",
    "                col_profile[\"examples\"] = series.dropna().sample(\n",
    "                    min(sample_size, series.dropna().shape[0]),\n",
    "                    random_state=42\n",
    "                ).tolist()\n",
    "            except ValueError:\n",
    "                col_profile[\"examples\"] = []\n",
    "\n",
    "            # Top frequent values\n",
    "            if not series.isna().all():\n",
    "                top_freq = series.value_counts(dropna=True).head(freq_top)\n",
    "                col_profile[\"top_values\"] = {str(k): int(v) for k, v in top_freq.to_dict().items()}\n",
    "\n",
    "            # Numeric columns\n",
    "            if pd.api.types.is_numeric_dtype(series):\n",
    "                col_profile.update({\n",
    "                    \"min\": float(series.min(skipna=True)),\n",
    "                    \"max\": float(series.max(skipna=True)),\n",
    "                    \"mean\": float(series.mean(skipna=True)),\n",
    "                    \"std\": float(series.std(skipna=True))\n",
    "                })\n",
    "                if series.std(skipna=True) > 0:\n",
    "                    z_scores = ((series - series.mean(skipna=True)) / series.std(skipna=True)).abs()\n",
    "                    col_profile[\"has_outliers\"] = bool((z_scores > 3).any())\n",
    "                else:\n",
    "                    col_profile[\"has_outliers\"] = False\n",
    "\n",
    "            # Datetime columns\n",
    "            elif pd.api.types.is_datetime64_any_dtype(series):\n",
    "                if not series.dropna().empty:\n",
    "                    col_profile[\"min_date\"] = str(series.min())\n",
    "                    col_profile[\"max_date\"] = str(series.max())\n",
    "\n",
    "            # Text/categorical columns\n",
    "            elif pd.api.types.is_object_dtype(series):\n",
    "                lengths = series.dropna().astype(str).map(len)\n",
    "                if not lengths.empty:\n",
    "                    col_profile[\"avg_length\"] = float(lengths.mean())\n",
    "                    col_profile[\"max_length\"] = int(lengths.max())\n",
    "                unusual = series.dropna().astype(str).str.contains(r\"[^a-zA-Z0-9\\s]\", regex=True).sum()\n",
    "                col_profile[\"unusual_char_percent\"] = round(float(unusual / total * 100), 2)\n",
    "\n",
    "            # Flags\n",
    "            if series.nunique(dropna=True) == total:\n",
    "                col_profile[\"is_identifier\"] = True\n",
    "            elif series.nunique(dropna=True) <= 1:\n",
    "                col_profile[\"is_constant\"] = True\n",
    "\n",
    "            batch_profile[\"columns\"][col] = col_profile\n",
    "\n",
    "        fact_sheet[\"batches\"].append(batch_profile)\n",
    "\n",
    "    return fact_sheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e10c0ea4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_rows': 234005,\n",
       " 'n_columns': 7,\n",
       " 'batches': [{'columns': {'Unnamed: 0': {'dtype': 'int64',\n",
       "     'null_percent': 0.0,\n",
       "     'unique_percent': 100.0,\n",
       "     'examples': [254689, 147840, 321961],\n",
       "     'top_values': {'0': 1, '1': 1, '4': 1},\n",
       "     'min': 0.0,\n",
       "     'max': 511395.0,\n",
       "     'mean': 255205.03585393474,\n",
       "     'std': 147501.62599208727,\n",
       "     'has_outliers': False,\n",
       "     'is_identifier': True},\n",
       "    'date': {'dtype': 'object',\n",
       "     'null_percent': 0.0,\n",
       "     'unique_percent': 0.26,\n",
       "     'examples': ['2021-11-04', '2021-07-19', '2022-03-07'],\n",
       "     'top_values': {'2022-08-14': 997, '2021-08-15': 961, '2022-07-17': 941},\n",
       "     'avg_length': 10.0,\n",
       "     'max_length': 10,\n",
       "     'unusual_char_percent': 100.0},\n",
       "    'time': {'dtype': 'object',\n",
       "     'null_percent': 0.0,\n",
       "     'unique_percent': 0.29,\n",
       "     'examples': ['12:37', '19:14', '16:20'],\n",
       "     'top_values': {'11:43': 859, '11:46': 850, '12:11': 850},\n",
       "     'avg_length': 5.0,\n",
       "     'max_length': 5,\n",
       "     'unusual_char_percent': 100.0},\n",
       "    'ticket_number': {'dtype': 'float64',\n",
       "     'null_percent': 0.0,\n",
       "     'unique_percent': 58.31,\n",
       "     'examples': [218841.0, 190030.0, 237182.0],\n",
       "     'top_values': {'225766.0': 13, '202116.0': 12, '155163.0': 12},\n",
       "     'min': 150040.0,\n",
       "     'max': 288913.0,\n",
       "     'mean': 219201.25873806115,\n",
       "     'std': 40053.22389631496,\n",
       "     'has_outliers': False},\n",
       "    'article': {'dtype': 'object',\n",
       "     'null_percent': 0.0,\n",
       "     'unique_percent': 0.06,\n",
       "     'examples': ['MOISSON', 'COUPE', 'BAGUETTE'],\n",
       "     'top_values': {'TRADITIONAL BAGUETTE': 67689,\n",
       "      'COUPE': 20470,\n",
       "      'BAGUETTE': 15292},\n",
       "     'avg_length': 12.91998461571334,\n",
       "     'max_length': 24,\n",
       "     'unusual_char_percent': 0.26},\n",
       "    'Quantity': {'dtype': 'float64',\n",
       "     'null_percent': 0.0,\n",
       "     'unique_percent': 0.02,\n",
       "     'examples': [1.0, 1.0, 1.0],\n",
       "     'top_values': {'1.0': 156712, '2.0': 49367, '3.0': 13677},\n",
       "     'min': -200.0,\n",
       "     'max': 200.0,\n",
       "     'mean': 1.538377385098609,\n",
       "     'std': 1.2896033140596856,\n",
       "     'has_outliers': True},\n",
       "    'unit_price': {'dtype': 'object',\n",
       "     'null_percent': 0.0,\n",
       "     'unique_percent': 0.05,\n",
       "     'examples': ['2,00 €', '0,15 €', '0,95 €'],\n",
       "     'top_values': {'1,20 €': 49080, '0,15 €': 20471, '1,30 €': 19778},\n",
       "     'avg_length': 6.006734898826948,\n",
       "     'max_length': 7,\n",
       "     'unusual_char_percent': 100.0}}}]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eda_fact_sheet_bakery(r\"D:\\Code Assistant\\Bakery sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d79b1adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eda_fact_sheet_bakery(sample_size: int = 3, freq_top: int = 3):\n",
    "    \"\"\"\n",
    "    Returns a fact sheet for the 'Bakery sales.csv' dataset.\n",
    "    \n",
    "    Args:\n",
    "        sample_size (int): Number of random sample values to show per column.\n",
    "        freq_top (int): Number of top frequent values to display.\n",
    "    \"\"\"\n",
    "    path = r\"D:\\Code Assistant\\Bakery sales.csv\"  # dataset path\n",
    "\n",
    "    # --- load dataset ---\n",
    "    if path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(path)\n",
    "    elif path.endswith(\".xlsx\") or path.endswith(\".xls\"):\n",
    "        df = pd.read_excel(path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Use CSV or Excel.\")\n",
    "\n",
    "    fact_sheet = {\n",
    "        \"n_rows\": len(df),\n",
    "        \"n_columns\": len(df.columns),\n",
    "        \"batches\": []\n",
    "    }\n",
    "\n",
    "    # --- decide split logic ---\n",
    "    n_cols = len(df.columns)\n",
    "    if n_cols <= 15:\n",
    "        batch_splits = [df.columns.tolist()]\n",
    "    elif n_cols <= 30:\n",
    "        mid = n_cols // 2\n",
    "        batch_splits = [df.columns[:mid].tolist(), df.columns[mid:].tolist()]\n",
    "    else:\n",
    "        batch_splits = [df.columns[i:i+15].tolist() for i in range(0, n_cols, 15)]\n",
    "\n",
    "    # --- process each batch ---\n",
    "    for batch_cols in batch_splits:\n",
    "        batch_profile = {\"columns\": {}}\n",
    "        for col in batch_cols:\n",
    "            series = df[col]\n",
    "            col_profile = {}\n",
    "            total = len(series)\n",
    "\n",
    "            # Core stats\n",
    "            col_profile[\"dtype\"] = str(series.dtype)\n",
    "            col_profile[\"null_percent\"] = round((series.isna().sum() / total) * 100, 2)\n",
    "            col_profile[\"unique_percent\"] = round((series.nunique(dropna=True) / total) * 100, 2)\n",
    "\n",
    "            # Example values\n",
    "            try:\n",
    "                col_profile[\"examples\"] = series.dropna().sample(\n",
    "                    min(sample_size, series.dropna().shape[0]),\n",
    "                    random_state=42\n",
    "                ).tolist()\n",
    "            except ValueError:\n",
    "                col_profile[\"examples\"] = []\n",
    "\n",
    "            # Top frequent values\n",
    "            if not series.isna().all():\n",
    "                top_freq = series.value_counts(dropna=True).head(freq_top)\n",
    "                col_profile[\"top_values\"] = top_freq.to_dict()\n",
    "\n",
    "            # Numeric columns\n",
    "            if pd.api.types.is_numeric_dtype(series):\n",
    "                col_profile.update({\n",
    "                    \"min\": float(series.min(skipna=True)),\n",
    "                    \"max\": float(series.max(skipna=True)),\n",
    "                    \"mean\": float(series.mean(skipna=True)),\n",
    "                    \"std\": float(series.std(skipna=True))\n",
    "                })\n",
    "                if series.std(skipna=True) > 0:\n",
    "                    z_scores = ((series - series.mean(skipna=True)) / series.std(skipna=True)).abs()\n",
    "                    col_profile[\"has_outliers\"] = bool((z_scores > 3).any())\n",
    "                else:\n",
    "                    col_profile[\"has_outliers\"] = False\n",
    "\n",
    "            # Datetime columns\n",
    "            elif pd.api.types.is_datetime64_any_dtype(series):\n",
    "                if not series.dropna().empty:\n",
    "                    col_profile[\"min_date\"] = str(series.min())\n",
    "                    col_profile[\"max_date\"] = str(series.max())\n",
    "\n",
    "            # Text/categorical columns\n",
    "            elif pd.api.types.is_object_dtype(series):\n",
    "                lengths = series.dropna().astype(str).map(len)\n",
    "                if not lengths.empty:\n",
    "                    col_profile[\"avg_length\"] = float(lengths.mean())\n",
    "                    col_profile[\"max_length\"] = int(lengths.max())\n",
    "                unusual = series.dropna().astype(str).str.contains(r\"[^a-zA-Z0-9\\s]\", regex=True).sum()\n",
    "                col_profile[\"unusual_char_percent\"] = round((unusual / total) * 100, 2)\n",
    "\n",
    "            # Flags\n",
    "            if series.nunique(dropna=True) == total:\n",
    "                col_profile[\"is_identifier\"] = True\n",
    "            elif series.nunique(dropna=True) <= 1:\n",
    "                col_profile[\"is_constant\"] = True\n",
    "\n",
    "            batch_profile[\"columns\"][col] = col_profile\n",
    "\n",
    "        fact_sheet[\"batches\"].append(batch_profile)\n",
    "\n",
    "    return fact_sheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb01335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "155309ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_rows': 234005, 'n_columns': 7, 'batches': [{'columns': {'Unnamed: 0': {'dtype': 'int64', 'null_percent': np.float64(0.0), 'unique_percent': 100.0, 'examples': [254689, 147840, 321961], 'top_values': {0: 1, 1: 1, 4: 1}, 'min': 0.0, 'max': 511395.0, 'mean': 255205.03585393474, 'std': 147501.62599208727, 'has_outliers': False, 'is_identifier': True}, 'date': {'dtype': 'object', 'null_percent': np.float64(0.0), 'unique_percent': 0.26, 'examples': ['2021-11-04', '2021-07-19', '2022-03-07'], 'top_values': {'2022-08-14': 997, '2021-08-15': 961, '2022-07-17': 941}, 'avg_length': 10.0, 'max_length': 10, 'unusual_char_percent': np.float64(100.0)}, 'time': {'dtype': 'object', 'null_percent': np.float64(0.0), 'unique_percent': 0.29, 'examples': ['12:37', '19:14', '16:20'], 'top_values': {'11:43': 859, '11:46': 850, '12:11': 850}, 'avg_length': 5.0, 'max_length': 5, 'unusual_char_percent': np.float64(100.0)}, 'ticket_number': {'dtype': 'float64', 'null_percent': np.float64(0.0), 'unique_percent': 58.31, 'examples': [218841.0, 190030.0, 237182.0], 'top_values': {225766.0: 13, 202116.0: 12, 155163.0: 12}, 'min': 150040.0, 'max': 288913.0, 'mean': 219201.25873806115, 'std': 40053.22389631496, 'has_outliers': False}, 'article': {'dtype': 'object', 'null_percent': np.float64(0.0), 'unique_percent': 0.06, 'examples': ['MOISSON', 'COUPE', 'BAGUETTE'], 'top_values': {'TRADITIONAL BAGUETTE': 67689, 'COUPE': 20470, 'BAGUETTE': 15292}, 'avg_length': 12.91998461571334, 'max_length': 24, 'unusual_char_percent': np.float64(0.26)}, 'Quantity': {'dtype': 'float64', 'null_percent': np.float64(0.0), 'unique_percent': 0.02, 'examples': [1.0, 1.0, 1.0], 'top_values': {1.0: 156712, 2.0: 49367, 3.0: 13677}, 'min': -200.0, 'max': 200.0, 'mean': 1.538377385098609, 'std': 1.2896033140596856, 'has_outliers': True}, 'unit_price': {'dtype': 'object', 'null_percent': np.float64(0.0), 'unique_percent': 0.05, 'examples': ['2,00 €', '0,15 €', '0,95 €'], 'top_values': {'1,20 €': 49080, '0,15 €': 20471, '1,30 €': 19778}, 'avg_length': 6.006734898826948, 'max_length': 7, 'unusual_char_percent': np.float64(100.0)}}}]}\n"
     ]
    }
   ],
   "source": [
    "fact_sheet = eda_fact_sheet_bakery()\n",
    "print(fact_sheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2a8602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(temperature=0, model_name=\"openai/gpt-oss-120b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82232a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant functionality\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_tavily import TavilySearch\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Create the agent\n",
    "memory = MemorySaver()\n",
    "model = llm\n",
    "search = TavilySearch(max_results=2)\n",
    "tools = [search]\n",
    "agent_executor = create_react_agent(model, tools, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb1a3fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hi, I'm Bob and I live in SF.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! 👋 Great to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Use the agent\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "input_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Hi, I'm Bob and I live in SF.\",\n",
    "}\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [input_message]}, config, stream_mode=\"values\"\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d1b9914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What's my name and where do i live?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You introduced yourself as **Bob**, and you mentioned that you live in **San Francisco (SF)**.\n"
     ]
    }
   ],
   "source": [
    "input_message = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"What's my name and where do i live?\",\n",
    "}\n",
    "\n",
    "for step in agent_executor.stream(\n",
    "    {\"messages\": [input_message]}, config, stream_mode=\"values\"\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1c44f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "  os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter API key for Groq: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\"openai/gpt-oss-120b\", model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3b5e365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I help you today?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hi!\"\n",
    "response = model.invoke([{\"role\": \"user\", \"content\": query}])\n",
    "response.text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46488eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Bakery sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8e23c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "class PythonInputs(BaseModel):\n",
    "    query: str = Field(description=\"A valid python command to run.\")\n",
    "\n",
    "# 3. Create the tool using the @tool decorator. This is the gold standard.\n",
    "@tool(args_schema=PythonInputs)\n",
    "def python_repl_ast(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Runs a Python command and returns the result.\n",
    "    The command has access to a pandas DataFrame named `df`.\n",
    "    \"\"\"\n",
    "    # Create a restricted namespace for the execution to see the DataFrame\n",
    "    local_namespace = {\"df\": df}\n",
    "    global_namespace = {}\n",
    "\n",
    "    # Capture stdout to return print() statements\n",
    "    old_stdout = sys.stdout\n",
    "    sys.stdout = captured_output = StringIO()\n",
    "\n",
    "    try:\n",
    "        # Execute the code\n",
    "        exec(query, global_namespace, local_namespace)\n",
    "        \n",
    "        # Get the output\n",
    "        output = captured_output.getvalue()\n",
    "        if output:\n",
    "            return output\n",
    "        else:\n",
    "            # If nothing was printed, try to eval the last line for a result\n",
    "            try:\n",
    "                # Split the query into lines and eval the last non-empty one\n",
    "                lines = [line for line in query.strip().split('\\n') if line.strip()]\n",
    "                if lines:\n",
    "                    last_line_result = eval(lines[-1], global_namespace, local_namespace)\n",
    "                    return str(last_line_result)\n",
    "                else:\n",
    "                    return \"Executed successfully, but no output was produced.\"\n",
    "            except Exception:\n",
    "                 return \"Executed successfully, but no output was produced.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Execution failed with error: {e}\"\n",
    "    finally:\n",
    "        # Restore stdout\n",
    "        sys.stdout = old_stdout\n",
    "\n",
    "tools = [python_repl_ast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fff3d56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "21f60152",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mGive the overview of the dataframe which the tool has access and the preprocessing steps needed to perform EDA on .\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m response = model_with_tools.invoke([{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: query}])\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMessage content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "query = \"Give the overview of the dataframe which the tool has access and the preprocessing steps needed to perform EDA on .\"\n",
    "response = model_with_tools.invoke([{\"role\": \"user\", \"content\": query}])\n",
    "\n",
    "print(f\"Message content: {response.content()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3584b90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calls: [{'name': 'python_repl_ast', 'args': {'query': \"import pandas as pd\\nprint('Shape:', df.shape)\\nprint('\\\\nColumns:', df.columns.tolist())\\nprint('\\\\nHead:')\\nprint(df.head())\\nprint('\\\\nInfo:')\\nprint(df.info())\\nprint('\\\\nDescribe:')\\nprint(df.describe(include='all'))\"}, 'id': 'fc_4d928c8c-d74b-47db-b093-c5dac571064e', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tool calls: {response.tool_calls}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "266e239b",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"'messages' : minimum number of items is 1\", 'type': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# 5. Run query\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m     50\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mGive the overview of the dataframe `df` and the preprocessing steps needed for EDA.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m response = \u001b[43magent_executor\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGive the overview of the dataframe `df` and the preprocessing steps needed for EDA.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfigurable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthread_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpreprocess-session-1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== FINAL OUTPUT ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     58\u001b[39m \u001b[38;5;28mprint\u001b[39m(response[\u001b[33m\"\u001b[39m\u001b[33moutput\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3026\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3023\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3024\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3026\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3027\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3031\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3032\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3034\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3035\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3036\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3038\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3039\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3040\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3041\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2647\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2645\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2646\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2647\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2653\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2654\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2657\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:162\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    160\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:394\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    392\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m         ret = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    396\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\langgraph\\prebuilt\\chat_agent_executor.py:627\u001b[39m, in \u001b[36mcreate_react_agent.<locals>.call_model\u001b[39m\u001b[34m(state, runtime, config)\u001b[39m\n\u001b[32m    625\u001b[39m     response = cast(AIMessage, dynamic_model.invoke(model_input, config))  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     response = cast(AIMessage, \u001b[43mstatic_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m    629\u001b[39m \u001b[38;5;66;03m# add agent name to the AIMessage\u001b[39;00m\n\u001b[32m    630\u001b[39m response.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3245\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3243\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3244\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3245\u001b[39m                 input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3246\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3247\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5710\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5703\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5704\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5705\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5708\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5709\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5710\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5711\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5712\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5713\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5714\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1023\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1014\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1016\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1020\u001b[39m     **kwargs: Any,\n\u001b[32m   1021\u001b[39m ) -> LLMResult:\n\u001b[32m   1022\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:840\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    838\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    839\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m840\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    846\u001b[39m         )\n\u001b[32m    847\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    848\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1089\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1087\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1088\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1089\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1093\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\langchain_groq\\chat_models.py:533\u001b[39m, in \u001b[36mChatGroq._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    528\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    529\u001b[39m params = {\n\u001b[32m    530\u001b[39m     **params,\n\u001b[32m    531\u001b[39m     **kwargs,\n\u001b[32m    532\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:448\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, compound_custom, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    239\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    240\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    295\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    296\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    297\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    298\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    299\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    446\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    447\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code Assistant\\venv\\Lib\\site-packages\\groq\\_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"'messages' : minimum number of items is 1\", 'type': 'invalid_request_error'}}",
      "During task with name 'agent' and id '86cec74b-8de0-333f-a9bc-70cb56e8a6a1'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# -------------------------\n",
    "# 1. Dummy DataFrame\n",
    "# -------------------------\n",
    "data = {\n",
    "    \"age\": [25, 30, None, 40, 35],\n",
    "    \"income\": [50000, 60000, 55000, None, 65000],\n",
    "    \"gender\": [\"M\", \"F\", \"F\", \"M\", None]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# -------------------------\n",
    "# 2. Define a Python tool\n",
    "# -------------------------\n",
    "@tool\n",
    "def python_repl_ast(query: str) -> str:\n",
    "    \"\"\"Run Python code with access to a pandas DataFrame `df`.\"\"\"\n",
    "    local_namespace = {\"df\": df}\n",
    "    try:\n",
    "        exec(query, {}, local_namespace)\n",
    "        # Capture last expression if exists\n",
    "        result = local_namespace.get(\"result\", \"Executed successfully.\")\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# -------------------------\n",
    "# 3. Init LLM\n",
    "# -------------------------\n",
    "# pick a free model wrapper you have (e.g. Ollama, HuggingFace, or OpenAI key)\n",
    "# here I’ll assume you’re using an OpenAI-compatible model\n",
    "llm = ChatGroq(temperature=0, model_name=\"openai/gpt-oss-120b\")\n",
    "\n",
    "# -------------------------\n",
    "# 4. Create ReAct agent\n",
    "# -------------------------\n",
    "memory = MemorySaver()\n",
    "tools = [python_repl_ast]\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)\n",
    "\n",
    "# -------------------------\n",
    "# 5. Run query\n",
    "# -------------------------\n",
    "query = \"Give the overview of the dataframe `df` and the preprocessing steps needed for EDA.\"\n",
    "response = agent_executor.invoke(\n",
    "    {\"input\": \"Give the overview of the dataframe `df` and the preprocessing steps needed for EDA.\"},\n",
    "    config={\"configurable\": {\"thread_id\": \"preprocess-session-1\"}}\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n=== FINAL OUTPUT ===\")\n",
    "print(response[\"output\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01f78bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Query ---\n",
      "User Query: Give me a summary of the dataframe, including the data types and null values.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "An error occurred: Tool choice is none, but model called a tool\n",
      "--- Running Query ---\n",
      "User Query: What is the total revenue from all sales?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "An error occurred: Tool choice is none, but model called a tool\n",
      "--- Running Query ---\n",
      "User Query: Which product had the highest quantity sold?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "An error occurred: Tool choice is none, but model called a tool\n",
      "--- Running Query ---\n",
      "User Query: What was the average unit price for products in the 'Electronics' category?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "An error occurred: Tool choice is none, but model called a tool\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# --- Configuration ---\n",
    "# 1. Get your Groq API key from https://console.groq.com/keys\n",
    "# 2. Set it as an environment variable\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "# --- Create a Sample DataFrame ---\n",
    "data = {\n",
    "    'product_name': ['Laptop', 'Mouse', 'Keyboard', 'Monitor', 'Webcam'],\n",
    "    'category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Accessories'],\n",
    "    'quantity_sold': [120, 350, 275, 150, 400],\n",
    "    'unit_price': [1200, 25, 75, 300, 50],\n",
    "    'sale_date': pd.to_datetime(['2023-01-15', '2023-01-20', '2023-02-10', '2023-02-12', '2023-03-05'])\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df['total_revenue'] = df['quantity_sold'] * df['unit_price']\n",
    "\n",
    "\n",
    "# --- Initialize the Groq LLM ---\n",
    "# Using a model like Llama3 8b is a good choice for speed and capability\n",
    "# You can also use other models available on Groq, like \"mixtral-8x7b-32768\"\n",
    "llm = ChatGroq(temperature=0, model_name=\"openai/gpt-oss-120b\")\n",
    "\n",
    "# --- Create the Pandas DataFrame Agent ---\n",
    "# Note: allow_dangerous_code=True is required for the agent to execute python code.\n",
    "# This should only be used in a secure, sandboxed environment.\n",
    "agent = create_pandas_dataframe_agent(\n",
    "    llm,\n",
    "    df,\n",
    "    verbose=True,\n",
    "    allow_dangerous_code=True\n",
    ")\n",
    "\n",
    "# --- Define Queries for the Agent to Perform ---\n",
    "queries = [\n",
    "    \"Give me a summary of the dataframe, including the data types and null values.\",\n",
    "    \"What is the total revenue from all sales?\",\n",
    "    \"Which product had the highest quantity sold?\",\n",
    "    \"What was the average unit price for products in the 'Electronics' category?\",\n",
    "]\n",
    "\n",
    "# --- Run the Queries ---\n",
    "for query in queries:\n",
    "    print(f\"--- Running Query ---\")\n",
    "    print(f\"User Query: {query}\")\n",
    "    try:\n",
    "        response = agent.invoke(query)\n",
    "        print(f\"\\nAgent Response: {response['output']}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b208a483",
   "metadata": {},
   "outputs": [],
   "source": [
    "You are absolutely right. My apologies. In a true multi-agent supervisor pattern, the supervisor LLM must be the one making the routing decisions. A deterministic Python function breaks that pattern.\n",
    "\n",
    "The problem you're facing is a fascinating one: the LLM is correctly reasoning but failing at the last moment by hallucinating the tool name (json instead of Router). This is a known issue with some models when using the with_structured_output method.\n",
    "\n",
    "Let's force the LLM to do the right thing. There are two primary strategies to make this work reliably.\n",
    "\n",
    "Strategy 1: (Recommended) Bind the Tool Explicitly to the LLM\n",
    "\n",
    "This is the most modern and robust way to handle tool-calling with LangChain. Instead of just asking for structured output, you explicitly bind the tool (your Router Pydantic model) to the LLM. This gives the model a much stronger signal about the exact tool it is expected to use.\n",
    "\n",
    "This method is less prone to the \"hallucinated name\" error because the model is primed to use the specific tool you've given it.\n",
    "\n",
    "Here is how you modify your supervisor_node:\n",
    "\n",
    "code\n",
    "Python\n",
    "download\n",
    "content_copy\n",
    "expand_less\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "# Make sure your Router model is defined correctly\n",
    "class Router(BaseModel):\n",
    "    \"\"\"Represents the routing decision and the reasoning behind it.\"\"\"\n",
    "    next: Literal[\"Analyzer_node\", \"__end__\"] = Field(\n",
    "        ...,\n",
    "        description=\"The next node to route to. Must be one of 'Analyzer_node' or '__end__'.\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        ...,\n",
    "        description=\"A concise explanation of why this routing decision was made.\"\n",
    "    )\n",
    "\n",
    "# ... inside your supervisor_node ...\n",
    "def supervisor_node(self, state: AgentStateModel):\n",
    "    print(\"...\")\n",
    "\n",
    "    # --- NEW: Bind the Router tool to the LLM ---\n",
    "    # This creates a new runnable that is primed to use the Router tool.\n",
    "    llm_with_router_tool = self.tool_disabled_llm.bind_tools([Router])\n",
    "\n",
    "    # Your prompt can be simplified, as the model now knows its job is to call the tool.\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are the supervisor. Your job is to review the current state and decide the next step. \"\n",
    "                   \"You must call the `Router` tool to indicate your decision.\"),\n",
    "        (\"user\", \"Here is the current state of the 'Analysis' field: {analysis_state}. \"\n",
    "                 \"If the field is an empty list (`[]`), route to 'Analyzer_node'. \"\n",
    "                 \"If it contains any data, route to '__end__'. Provide your reasoning.\"),\n",
    "    ])\n",
    "\n",
    "    # Create the chain: prompt -> LLM with tool -> (optional) parser\n",
    "    # The output of the LLM will now be a message containing a `tool_calls` attribute.\n",
    "    chain = prompt | llm_with_router_tool\n",
    "\n",
    "    print(\"***********************Invoking LLM for routing decision************************\")\n",
    "    \n",
    "    # Invoke the chain with the current state analysis\n",
    "    response_message = chain.invoke({\n",
    "        \"analysis_state\": str(state.Analysis) # Pass the state as a string for clarity\n",
    "    })\n",
    "\n",
    "    # --- NEW: Extract the arguments from the tool call ---\n",
    "    # The response is now a message object, not the parsed output directly.\n",
    "    tool_call = response_message.tool_calls[0]\n",
    "    if tool_call['name'] == \"Router\":\n",
    "        routing_decision = Router(**tool_call['args'])\n",
    "        goto = routing_decision.next\n",
    "        reasoning = routing_decision.reasoning\n",
    "    else:\n",
    "        # Fallback in case of another hallucination, though less likely now\n",
    "        print(f\"Warning: LLM called an unexpected tool: {tool_call['name']}\")\n",
    "        goto = \"__end__\" \n",
    "        reasoning = \"LLM failed to call the correct Router tool.\"\n",
    "\n",
    "    print(f\"********************************this is my goto*************************\\n{goto}\")\n",
    "    print(f\"********************************\\n{reasoning}\")\n",
    "\n",
    "    if goto == \"__end__\":\n",
    "        goto = END\n",
    "\n",
    "    return Command(\n",
    "        goto=goto,\n",
    "        update={'next': goto, 'current_reasoning': reasoning}\n",
    "    )\n",
    "\n",
    "Why this works better:\n",
    "\n",
    ".bind_tools([Router]) explicitly tells the Groq API that the Router tool is available for this call. The LLM is heavily incentivized to use it.\n",
    "\n",
    "The output is a standard AIMessage with a tool_calls attribute, which is a more standard and less \"magical\" way of handling tool use than with_structured_output.\n",
    "\n",
    "We manually parse the arguments from the tool call, giving us full control and visibility.\n",
    "\n",
    "Strategy 2: Force JSON Mode with a Specific Prompt\n",
    "\n",
    "If you want to stick closer to your original code, you can try to force the model into a JSON-only output mode with a very strong prompt. This is essentially \"prompt engineering\" a solution.\n",
    "\n",
    "code\n",
    "Python\n",
    "download\n",
    "content_copy\n",
    "expand_less\n",
    "IGNORE_WHEN_COPYING_START\n",
    "IGNORE_WHEN_COPYING_END\n",
    "# ... inside your supervisor_node ...\n",
    "def supervisor_node(self, state: AgentStateModel):\n",
    "    print(\"...\")\n",
    "\n",
    "    # Your Router Pydantic model is the same.\n",
    "    # We will use it with with_structured_output.\n",
    "\n",
    "    # --- NEW: A much more forceful prompt ---\n",
    "    # We explicitly tell the model its output format.\n",
    "    system_prompt = (\n",
    "        \"You are a supervisor agent. Your sole purpose is to act as a JSON router. \"\n",
    "        \"You will be given the current state of a task. \"\n",
    "        \"You must respond with ONLY a valid JSON object that conforms to the following Pydantic schema: \"\n",
    "        \"```json\\n\"\n",
    "        \"{\\\"title\\\": \\\"Router\\\", \\\"description\\\": \\\"...\\\", \\\"properties\\\": {\\\"next\\\": ..., \\\"reasoning\\\": ...}}\\n\"\n",
    "        \"```\\n\"\n",
    "        \"Do not add any other text, explanation, or markdown formatting around the JSON object.\"\n",
    "    )\n",
    "    \n",
    "    human_prompt = (\n",
    "        f\"Current 'Analysis' state: {state.Analysis}. \"\n",
    "        \"If 'Analysis' is empty, set 'next' to 'Analyzer_node'. \"\n",
    "        \"If 'Analysis' contains data, set 'next' to '__end__'. \"\n",
    "        \"Provide your reasoning in the 'reasoning' field.\"\n",
    "    )\n",
    "\n",
    "    messages_for_llm = [\n",
    "        SystemMessage(content=system_prompt),\n",
    "        HumanMessage(content=human_prompt),\n",
    "    ]\n",
    "\n",
    "    print(\"***********************Invoking LLM for routing decision************************\")\n",
    "    \n",
    "    # The invoke call remains the same\n",
    "    response = self.tool_disabled_llm.with_structured_output(Router).invoke(messages_for_llm)\n",
    "    \n",
    "    goto = response.next\n",
    "    reasoning = response.reasoning\n",
    "\n",
    "    print(f\"********************************this is my goto*************************\\n{goto}\")\n",
    "    print(f\"********************************\\n{reasoning}\")\n",
    "\n",
    "    if goto == \"__end__\":\n",
    "        goto = END\n",
    "\n",
    "    return Command(\n",
    "        goto=goto,\n",
    "        update={'next': goto, 'current_reasoning': reasoning}\n",
    "    )\n",
    "\n",
    "Why this might work:\n",
    "\n",
    "By explicitly telling the model to only output JSON and even showing it a schema snippet, you reduce its creative freedom and nudge it away from thinking it needs to wrap its output in a tool call.\n",
    "\n",
    "This approach relies more on the model's ability to follow instructions perfectly, which can still be brittle.\n",
    "\n",
    "Recommendation: Use Strategy 1 (bind_tools). It is the more modern, reliable, and idiomatic LangChain approach for this problem. It aligns with how models are being fine-tuned for tool use and is less susceptible to prompt-following errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
